{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78c733-0fd8-43db-a36b-ceb8c249f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "\n",
    "#print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'Amazon':\n",
    "        transactions = pd.read_csv('amazon_transactions.csv', encoding = 'ISO-8859-1')\n",
    "        items = pd.read_csv('amazon_items.csv', encoding = 'ISO-8859-1')\n",
    "    elif dataset_name == 'Best Buy':\n",
    "        transactions = pd.read_csv('bestbuy_transactions.csv', encoding = 'ISO-8859-1')\n",
    "        items = pd.read_csv('bestbuy_items.csv', encoding = 'ISO-8859-1')\n",
    "    elif dataset_name == 'Kmart':\n",
    "        transactions = pd.read_csv('kmart_transactions.csv', encoding = 'ISO-8859-1')\n",
    "        items = pd.read_csv('kmart_items.csv', encoding = 'ISO-8859-1')\n",
    "    elif dataset_name == 'Nike':\n",
    "        transactions = pd.read_csv('nike_transactions.csv', encoding = 'ISO-8859-1')\n",
    "        items = pd.read_csv('nike_items.csv', encoding = 'ISO-8859-1')\n",
    "    elif dataset_name == 'General':\n",
    "        transactions = pd.read_csv('general_transactions.csv', encoding = 'ISO-8859-1')\n",
    "        items = pd.read_csv('general_items.csv', encoding = 'ISO-8859-1')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name\")\n",
    "    return transactions, items\n",
    "\n",
    "\n",
    "def preprocess_transactions(transactions):\n",
    "    unique_items = sorted(set(itertools.chain.from_iterable(transactions['items'].apply(lambda x: x.split(',')))))\n",
    "    return unique_items\n",
    "\n",
    "def get_min_support_count(transactions, min_support):\n",
    "    return int(min_support * len(transactions))\n",
    "\n",
    "def generate_candidates(itemsets, length):\n",
    "    return set(itertools.combinations(itemsets, length))\n",
    "\n",
    "def count_itemsets(transactions, candidates):\n",
    "    itemset_count = {itemset: 0 for itemset in candidates}\n",
    "    for transaction in transactions['items']:\n",
    "        transaction_items = set(transaction.split(','))\n",
    "        for itemset in candidates:\n",
    "            if set(itemset).issubset(transaction_items):\n",
    "                itemset_count[itemset] += 1\n",
    "    return {itemset: count for itemset, count in itemset_count.items() if count > 0}\n",
    "\n",
    "def generate_frequent_itemsets(transactions, min_support_count):\n",
    "    itemsets = [tuple([item]) for item in preprocess_transactions(transactions)]\n",
    "    frequent_itemsets = {}\n",
    "    k = 1\n",
    "    max_k = len(set(item for transaction in transactions for item in transaction))  # Maximum itemset size\n",
    "\n",
    "    for k in range(1, max_k + 1):\n",
    "        candidates = generate_candidates(itemsets, k)\n",
    "        itemset_counts = count_itemsets(transactions, candidates)\n",
    "\n",
    "        # Update frequent itemsets based on support count\n",
    "        frequent_itemsets.update({\n",
    "            itemset: count \n",
    "            for itemset, count in itemset_counts.items() \n",
    "            if count >= min_support_count\n",
    "        })\n",
    "        \n",
    "        itemsets = list(itemset_counts.keys())\n",
    "        \n",
    "        # If there are no frequent itemsets left, we can break early\n",
    "        if not itemsets:\n",
    "            break\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for itemset, support in frequent_itemsets.items():\n",
    "        for i in range(1, len(itemset)):\n",
    "            for subset in itertools.combinations(itemset, i):\n",
    "                antecedent = subset\n",
    "                consequent = tuple(set(itemset) - set(antecedent))\n",
    "                if consequent:\n",
    "                    confidence = support / frequent_itemsets[antecedent]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence))\n",
    "    return rules\n",
    "\n",
    "def main():\n",
    "    dataset_name = input(\"Select a dataset (Amazon, Best Buy, Kmart, Nike, General): \")\n",
    "    transactions, items = load_data(dataset_name)\n",
    "    \n",
    "    min_support = float(input(\"Enter minimum support count (as a percentage): \")) / 100\n",
    "    min_confidence = float(input(\"Enter minimum confidence (as a decimal): \"))\n",
    "    \n",
    "    min_support_count = get_min_support_count(transactions, min_support)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    frequent_itemsets = generate_frequent_itemsets(transactions, min_support_count)\n",
    "    rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Frequent Itemsets:\")\n",
    "    for itemset, support in frequent_itemsets.items():\n",
    "        print(f\"{itemset}: {support}\")\n",
    "    \n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    for antecedent, consequent, confidence in rules:\n",
    "        print(f\"{antecedent} -> {consequent} (Confidence: {confidence})\")\n",
    "    \n",
    "    print(f\"\\nExecution Time: {end_time - start_time} seconds\")\n",
    "    \n",
    "    # Alternative Methods using mlxtend\n",
    "    transactions_encoded = pd.get_dummies(transactions['items'].str.get_dummies(sep=','))\n",
    "    frequent_itemsets_apriori = apriori(transactions_encoded, min_support=min_support, use_colnames=True)\n",
    "    frequent_itemsets_fpgrowth = fpgrowth(transactions_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "    print(\"\\nFrequent Itemsets using Apriori:\")\n",
    "    print(frequent_itemsets_apriori)\n",
    "    \n",
    "    print(\"\\nFrequent Itemsets using FPGrowth:\")\n",
    "    print(frequent_itemsets_fpgrowth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
